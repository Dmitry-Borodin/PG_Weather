#!/usr/bin/env python3
"""
–°–±–æ—Ä –ø—Ä–æ–≥–Ω–æ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç–µ–æ-—Ç—Ä–∏–∞–∂–∞ XC closed routes.
–í–µ—Ä—Å–∏—è: 0.6

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
  - Open-Meteo ICON-D2 (hi-res, 2 –∫–º)
  - Open-Meteo GFS (BL height, CAPE, CIN, LI)
  - Open-Meteo –º—É–ª—å—Ç–∏–º–æ–¥–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
  - GeoSphere NWP (–ø—Ä–æ–≥–Ω–æ–∑ CAPE, –æ–±–ª–∞—á–Ω–æ—Å—Ç—å, —Å–Ω–µ–≥–æ–≤–∞—è –ª–∏–Ω–∏—è)
  - BrightSky (DWD wrapper)
  - DWD MOSMIX (114 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±–ª–∞—á–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—è–º, —Ä–∞–¥–∏–∞—Ü–∏—è)
  - Meteo-Parapente (headless browser ‚Äî —Ç–µ—Ä–º–∏–∫–∏, –≤–µ—Ç–µ—Ä hi-res)
  - XContest (headless browser ‚Äî —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–ª—ë—Ç—ã, sanity check)
  - ALPTHERM (headless browser ‚Äî Thermikqualit√§t, –ê–≤—Å—Ç—Ä–∏—è)

–í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
  - JSON (–º–∞—à–∏–Ω–æ—á–∏—Ç–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ)
  - Markdown (—á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π –æ—Ç—á—ë—Ç)
  - HTML viewer (index.html ‚Äî –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö –æ—Ç—á—ë—Ç–æ–≤)
  - Self-contained HTML (latest.html + <forecast>_<timestamp>.html –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/fetch_weather.py                                  # —Å–ª–µ–¥. —Å—É–±–±–æ—Ç–∞, –≤—Å–µ
    python scripts/fetch_weather.py --date 2025-07-15                # –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –¥–∞—Ç–∞
    python scripts/fetch_weather.py --date 2025-07-15 --locations lenggries,koessen
"""

import argparse
import io
import json
import math
import subprocess
import shutil
import sys
import zipfile
from datetime import datetime, timedelta, timezone
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.parse import urlencode
from xml.etree import ElementTree as ET

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–π–æ–Ω–æ–≤
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

APP_VERSION = "0.6"

LOCATIONS = {
    "lenggries":   {"lat": 47.68, "lon": 11.57, "elev": 700,  "peaks": 1800, "name": "Lenggries",   "geosphere_id": None,    "mosmix_id": "10963", "drive_h": 1.0},
    "wallberg":    {"lat": 47.64, "lon": 11.79, "elev": 1620, "peaks": 1722, "name": "Wallberg",    "geosphere_id": None,    "mosmix_id": "10963", "drive_h": 1.0},
    "koessen":     {"lat": 47.67, "lon": 12.40, "elev": 590,  "peaks": 1900, "name": "K√∂ssen",      "geosphere_id": "11130", "mosmix_id": None,    "drive_h": 1.5},
    "innsbruck":   {"lat": 47.26, "lon": 11.39, "elev": 578,  "peaks": 2600, "name": "Innsbruck",   "geosphere_id": "11121", "mosmix_id": "11120", "drive_h": 2.0},
    "greifenburg": {"lat": 46.75, "lon": 13.18, "elev": 600,  "peaks": 2800, "name": "Greifenburg", "geosphere_id": "11204", "mosmix_id": None,    "drive_h": 4.0},
    "speikboden":  {"lat": 46.90, "lon": 11.87, "elev": 950,  "peaks": 2500, "name": "Speikboden",  "geosphere_id": None,    "mosmix_id": None,    "drive_h": 3.5},
    "bassano":     {"lat": 45.78, "lon": 11.73, "elev": 130,  "peaks": 1700, "name": "Bassano",     "geosphere_id": None,    "mosmix_id": None,    "drive_h": 5.0},
}

# –ß–∞—Å—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ—á–∞—Å–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è (Europe/Berlin)
ANALYSIS_HOURS = ["08:00", "09:00", "10:00", "11:00", "12:00",
                  "13:00", "14:00", "15:00", "16:00", "17:00", "18:00"]


def _next_saturday() -> str:
    """–ë–ª–∏–∂–∞–π—à–∞—è —Å—É–±–±–æ—Ç–∞ (–∏–ª–∏ —Å–µ–≥–æ–¥–Ω—è, –µ—Å–ª–∏ —Å—É–±–±–æ—Ç–∞). –§–æ—Ä–º–∞—Ç YYYY-MM-DD."""
    today = datetime.now().date()
    days_ahead = (5 - today.weekday()) % 7  # 5 = Saturday
    if days_ahead == 0 and today.weekday() != 5:
        days_ahead = 7
    return str(today + timedelta(days=days_ahead))


def _fetch_json(url: str, timeout: int = 30) -> dict:
    """HTTP GET ‚Üí JSON."""
    req = Request(url, headers={"User-Agent": f"PG-Weather-Triage/{APP_VERSION}"})
    with urlopen(req, timeout=timeout) as resp:
        return json.loads(resp.read().decode())


def _fetch_bytes(url: str, timeout: int = 30) -> bytes:
    """HTTP GET ‚Üí bytes."""
    req = Request(url, headers={"User-Agent": f"PG-Weather-Triage/{APP_VERSION}"})
    with urlopen(req, timeout=timeout) as resp:
        return resp.read()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò—Å—Ç–æ—á–Ω–∏–∫ 1: Open-Meteo ICON-D2 (hi-res)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

ICON_D2_PARAMS = [
    "temperature_2m", "dewpoint_2m",
    "windspeed_10m", "windgusts_10m", "winddirection_10m",
    "cloudcover", "cloudcover_low", "cloudcover_mid", "cloudcover_high",
    "precipitation", "cape",
    "temperature_850hPa", "temperature_700hPa",
    "windspeed_850hPa", "winddirection_850hPa",
    "windspeed_700hPa", "winddirection_700hPa",
]


def fetch_icon_d2(lat: float, lon: float, date: str) -> dict:
    """Open-Meteo ICON-D2 (2 –∫–º, –¥–æ +48—á)."""
    params = {
        "latitude": lat, "longitude": lon,
        "hourly": ",".join(ICON_D2_PARAMS),
        "models": "icon_d2",
        "start_date": date, "end_date": date,
        "timezone": "Europe/Berlin",
        "windspeed_unit": "ms",
    }
    return _fetch_json(f"https://api.open-meteo.com/v1/dwd-icon?{urlencode(params)}")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò—Å—Ç–æ—á–Ω–∏–∫ 2: Open-Meteo GFS (BL height, CAPE, CIN, LI)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

GFS_PARAMS = [
    "temperature_2m", "dewpoint_2m",
    "windspeed_10m", "windgusts_10m",
    "cape", "convective_inhibition", "lifted_index",
    "boundary_layer_height",
    "cloudcover", "precipitation",
    "shortwave_radiation",
    "temperature_850hPa", "temperature_700hPa", "temperature_500hPa",
    "windspeed_850hPa", "windspeed_700hPa",
    "winddirection_850hPa", "winddirection_700hPa",
]


def fetch_gfs(lat: float, lon: float, date: str) -> dict:
    """Open-Meteo GFS ‚Äî –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å BL height."""
    params = {
        "latitude": lat, "longitude": lon,
        "hourly": ",".join(GFS_PARAMS),
        "models": "gfs_seamless",
        "start_date": date, "end_date": date,
        "timezone": "Europe/Berlin",
        "windspeed_unit": "ms",
    }
    return _fetch_json(f"https://api.open-meteo.com/v1/gfs?{urlencode(params)}")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò—Å—Ç–æ—á–Ω–∏–∫ 3: Open-Meteo –º—É–ª—å—Ç–∏–º–æ–¥–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

MULTIMODEL_PARAMS = [
    "temperature_2m", "dewpoint_2m",
    "windspeed_10m", "windgusts_10m",
    "cape", "cloudcover", "precipitation",
]


def fetch_multimodel(lat: float, lon: float, date: str) -> dict:
    """ICON + GFS + ECMWF –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º."""
    params = {
        "latitude": lat, "longitude": lon,
        "hourly": ",".join(MULTIMODEL_PARAMS),
        "models": "icon_seamless,gfs_seamless,ecmwf_ifs025",
        "start_date": date, "end_date": date,
        "timezone": "Europe/Berlin",
        "windspeed_unit": "ms",
    }
    return _fetch_json(f"https://api.open-meteo.com/v1/forecast?{urlencode(params)}")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò—Å—Ç–æ—á–Ω–∏–∫ 4: GeoSphere NWP (AROME-based 2.5 –∫–º)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def fetch_geosphere_nwp(lat: float, lon: float) -> dict:
    """GeoSphere NWP –ø—Ä–æ–≥–Ω–æ–∑: CAPE, CIN, –æ–±–ª–∞—á–Ω–æ—Å—Ç—å, –≤–µ—Ç–µ—Ä, —Å–Ω–µ–≥–æ–≤–∞—è –ª–∏–Ω–∏—è."""
    url = (
        f"https://dataset.api.hub.geosphere.at/v1/timeseries/forecast/nwp-v1-1h-2500m"
        f"?parameters=t2m,cape,cin,tcc,u10m,v10m,ugust,vgust,snowlmt,rr_acc,grad"
        f"&lat_lon={lat},{lon}&output_format=geojson"
    )
    return _fetch_json(url)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò—Å—Ç–æ—á–Ω–∏–∫ 6: BrightSky (DWD wrapper)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def fetch_brightsky(lat: float, lon: float, date: str) -> dict:
    """BrightSky ‚Äî —É–¥–æ–±–Ω—ã–π JSON wrapper –¥–ª—è DWD MOSMIX."""
    url = f"https://api.brightsky.dev/weather?lat={lat}&lon={lon}&date={date}"
    return _fetch_json(url)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò—Å—Ç–æ—á–Ω–∏–∫ 7: DWD MOSMIX (KMZ ‚Üí XML)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

MOSMIX_PARAMS_OF_INTEREST = [
    "TTT",   # Temperature 2m (K)
    "Td",    # Dewpoint (K)
    "FF",    # Wind speed (m/s)
    "FX1",   # Max gust 1h (m/s)
    "DD",    # Wind direction (¬∞)
    "N",     # Total cloud cover (%)
    "Neff",  # Effective cloud cover (%)
    "Nh",    # High cloud cover (%)
    "Nm",    # Mid cloud cover (%)
    "Nl",    # Low cloud cover (%)
    "PPPP",  # Pressure (Pa)
    "SunD1", # Sunshine duration 1h (s)
    "Rad1h", # Global irradiance 1h (kJ/m¬≤)
    "RR1c",  # Precipitation 1h (kg/m¬≤)
    "wwP",   # Weather probability (%)
    "R101",  # Precip prob >0.1mm (%)
]


def fetch_mosmix(station_id: str, date: str) -> dict:
    """–°–∫–∞—á–∞—Ç—å MOSMIX_L KMZ –¥–ª—è —Å—Ç–∞–Ω—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –¥–∞—Ç—É."""
    url = (
        f"https://opendata.dwd.de/weather/local_forecasts/mos/MOSMIX_L/"
        f"single_stations/{station_id}/kml/MOSMIX_L_LATEST_{station_id}.kmz"
    )

    raw = _fetch_bytes(url, timeout=60)
    zf = zipfile.ZipFile(io.BytesIO(raw))
    kml_name = [n for n in zf.namelist() if n.endswith(".kml")][0]
    kml_data = zf.read(kml_name)

    ns = {
        "kml": "http://www.opengis.net/kml/2.2",
        "dwd": "https://opendata.dwd.de/weather/lib/pointforecast_dwd_extension_V1_0.xsd",
    }

    root = ET.fromstring(kml_data)

    # Timestamps
    time_steps = root.findall(".//dwd:ForecastTimeSteps/dwd:TimeStep", ns)
    timestamps = [ts.text for ts in time_steps]

    # Find indices for target date
    target_indices = {}
    for i, ts in enumerate(timestamps):
        if date in ts:
            hour_part = ts.split("T")[1][:5]  # "06:00"
            target_indices[hour_part] = i

    if not target_indices:
        return {"error": f"No data for date {date}", "station": station_id}

    # Extract parameters
    result = {"station": station_id, "timestamps": {}, "hourly": {}}
    result["timestamps"] = {h: timestamps[i] for h, i in target_indices.items()}

    placemark = root.find(".//kml:Placemark", ns)
    if placemark is None:
        return {"error": "No Placemark in KML", "station": station_id}

    station_name = placemark.findtext("kml:name", default=station_id, namespaces=ns)
    result["station_name"] = station_name.strip()

    forecasts = placemark.findall(".//dwd:Forecast", ns)
    for fc in forecasts:
        param_name = fc.get(f"{{{ns['dwd']}}}elementName")
        if param_name not in MOSMIX_PARAMS_OF_INTEREST:
            continue

        value_text = fc.findtext("dwd:value", default="", namespaces=ns)
        values = value_text.split()

        hourly_vals = {}
        for hour, idx in target_indices.items():
            if idx < len(values):
                raw_val = values[idx].strip()
                if raw_val == "-999.00" or raw_val == "-":
                    hourly_vals[hour] = None
                else:
                    try:
                        v = float(raw_val)
                        if param_name in ("TTT", "Td"):
                            v = round(v - 273.15, 1)
                        elif param_name == "PPPP":
                            v = round(v / 100, 1)
                        else:
                            v = round(v, 1)
                        hourly_vals[hour] = v
                    except ValueError:
                        hourly_vals[hour] = None
            else:
                hourly_vals[hour] = None

        result["hourly"][param_name] = hourly_vals

    return result


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –í—ã—á–∏—Å–ª–µ–Ω–∏—è
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def estimate_cloudbase_msl(temp_c: float, dewpoint_c: float, station_elev_m: float) -> float:
    """LCL ‚âà 125 √ó (T ‚àí Td) + elevation."""
    return 125.0 * (temp_c - dewpoint_c) + station_elev_m


def lapse_rate(t850: float, t700: float) -> float:
    """Lapse rate 850‚Üí700 –≥–ü–∞ [¬∞C/km]. –¢–∏–ø–∏—á–Ω–∞—è Œîz ‚âà 1.5 –∫–º."""
    return (t850 - t700) / 1.5


def estimate_wstar(bl_height_m: float, sw_radiation_wm2: float, temp_c: float) -> float:
    """Estimate W* (Deardorff convective velocity scale) [m/s].

    RASP-like thermal strength indicator.
    W* = (g / T_K * BL_h * H_s / (rho * cp))^(1/3)
    where H_s ‚âà 0.4 * shortwave_radiation (sensible heat fraction for dry mountain terrain).
    """
    if bl_height_m <= 10 or sw_radiation_wm2 <= 10:
        return 0.0
    g = 9.81       # m/s¬≤
    rho = 1.1      # kg/m¬≥ (approx at ~1000m)
    cp = 1005.0    # J/(kg¬∑K)
    T_K = temp_c + 273.15
    if T_K < 200:
        return 0.0
    H_s = 0.4 * sw_radiation_wm2  # sensible heat fraction
    arg = (g / T_K) * bl_height_m * H_s / (rho * cp)
    if arg <= 0:
        return 0.0
    return round(arg ** (1.0 / 3.0), 2)


def wind_from_uv(u: float, v: float) -> tuple:
    """(speed m/s, direction ¬∞) from u,v components."""
    speed = math.sqrt(u**2 + v**2)
    direction = (270 - math.degrees(math.atan2(v, u))) % 360
    return round(speed, 1), round(direction)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ü–æ—á–∞—Å–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ –æ–∫–Ω–∞
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def analyze_hourly_window(hourly_icon: dict, hourly_gfs: dict, loc: dict) -> dict:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ—á–∞—Å–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ –æ–∫–Ω–∞."""
    profile = []
    times_icon = hourly_icon.get("time", [])
    times_gfs = hourly_gfs.get("time", [])

    def _get(hourly, times, key, hour_str):
        for i, t in enumerate(times):
            if hour_str in t:
                vals = hourly.get(key, [])
                return vals[i] if i < len(vals) else None
        return None

    for hour in ANALYSIS_HOURS:
        t2m = _get(hourly_icon, times_icon, "temperature_2m", hour)
        td = _get(hourly_icon, times_icon, "dewpoint_2m", hour)
        cloud = _get(hourly_icon, times_icon, "cloudcover", hour)
        precip = _get(hourly_icon, times_icon, "precipitation", hour)
        ws850 = _get(hourly_icon, times_icon, "windspeed_850hPa", hour)
        gusts = _get(hourly_icon, times_icon, "windgusts_10m", hour)
        t850 = _get(hourly_icon, times_icon, "temperature_850hPa", hour)
        t700 = _get(hourly_icon, times_icon, "temperature_700hPa", hour)

        # Fallback to GFS
        if t2m is None:
            t2m = _get(hourly_gfs, times_gfs, "temperature_2m", hour)
        if td is None:
            td = _get(hourly_gfs, times_gfs, "dewpoint_2m", hour)
        if cloud is None:
            cloud = _get(hourly_gfs, times_gfs, "cloudcover", hour)
        if precip is None:
            precip = _get(hourly_gfs, times_gfs, "precipitation", hour)
        if ws850 is None:
            ws850 = _get(hourly_gfs, times_gfs, "windspeed_850hPa", hour)
        if gusts is None:
            gusts = _get(hourly_gfs, times_gfs, "windgusts_10m", hour)
        if t850 is None:
            t850 = _get(hourly_gfs, times_gfs, "temperature_850hPa", hour)
        if t700 is None:
            t700 = _get(hourly_gfs, times_gfs, "temperature_700hPa", hour)

        bl = _get(hourly_gfs, times_gfs, "boundary_layer_height", hour)
        cape = _get(hourly_gfs, times_gfs, "cape", hour)
        sw_rad = _get(hourly_gfs, times_gfs, "shortwave_radiation", hour)

        base_msl = None
        if t2m is not None and td is not None:
            base_msl = round(estimate_cloudbase_msl(t2m, td, loc["elev"]))

        lr = None
        if t850 is not None and t700 is not None:
            lr = round(lapse_rate(t850, t700), 1)

        wstar = None
        if bl is not None and sw_rad is not None and t2m is not None:
            wstar = estimate_wstar(bl, sw_rad, t2m)

        profile.append({
            "hour": hour,
            "temp_2m": t2m,
            "dewpoint": td,
            "cloudbase_msl": base_msl,
            "cloudcover": cloud,
            "precipitation": precip,
            "wind_850": ws850,
            "gusts": gusts,
            "lapse_rate": lr,
            "bl_height": bl,
            "cape": cape,
            "wstar": wstar,
        })

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ—Ä–º–∏—á–µ—Å–∫–æ–≥–æ –æ–∫–Ω–∞:
    #  - –ß–∞—Å —Å—á–∏—Ç–∞–µ—Ç—Å—è "—Ç–µ—Ä–º–∏—á–µ—Å–∫–∏–º" –¢–û–õ–¨–ö–û –µ—Å–ª–∏ W* >= 1.5 –º/—Å
    #  - –ò –±–∞–∑–∞ (margin –Ω–∞–¥ –ø–∏–∫–∞–º–∏) >= 1000 –º
    #  - –ò –Ω–µ—Ç –æ—Å–∞–¥–∫–æ–≤ (precipitation <= 0.1)
    peaks = loc["peaks"]
    thermal_hours = []

    for p in profile:
        h = int(p["hour"].split(":")[0])
        if h < 9:
            continue
        wstar = p.get("wstar")
        precip = p.get("precipitation")
        base = p.get("cloudbase_msl")

        # Must have W* >= 1.5
        if wstar is None or wstar < 1.5:
            continue
        # No precipitation
        if precip is not None and precip > 0.1:
            continue
        # Base margin must be >= 1000m over peaks
        if base is not None and (base - peaks) < 1000:
            continue

        thermal_hours.append(p)

    window = {"start": None, "end": None, "peak_hour": None,
              "duration_h": 0, "peak_lapse": None, "peak_cape": None}

    if thermal_hours:
        window["start"] = thermal_hours[0]["hour"]
        window["end"] = thermal_hours[-1]["hour"]
        window["duration_h"] = len(thermal_hours)

        best_lr = 0
        best_cape = 0
        peak_h = thermal_hours[0]["hour"]
        for th in thermal_hours:
            lr_val = th.get("lapse_rate") or 0
            cape_val = th.get("cape") or 0
            if lr_val > best_lr or (lr_val == best_lr and cape_val > best_cape):
                best_lr = lr_val
                best_cape = cape_val
                peak_h = th["hour"]
        window["peak_hour"] = peak_h
        window["peak_lapse"] = best_lr if best_lr > 0 else None
        window["peak_cape"] = best_cape if best_cape > 0 else None

    return {
        "hourly_profile": profile,
        "thermal_window": window,
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –°–±–æ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–¥–Ω–æ–π –ª–æ–∫–∞—Ü–∏–∏
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _get_at_hour(hourly: dict, hour: str, key: str):
    """–ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ hourly –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Å–∞."""
    times = hourly.get("time", [])
    values = hourly.get(key, [])
    for i, t in enumerate(times):
        if hour in t:
            return values[i] if i < len(values) else None
    return None


def assess_location(loc_key: str, loc: dict, date: str, sources: list) -> dict:
    """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–π –ª–æ–∫–∞—Ü–∏–∏."""
    result = {
        "location": loc["name"],
        "key": loc_key,
        "date": date,
        "drive_h": loc.get("drive_h", "?"),
        "sources": {},
        "assessment": {},
    }

    lat, lon = loc["lat"], loc["lon"]
    hourly_icon = {}
    hourly_gfs = {}

    # --- ICON-D2 ---
    if "icon_d2" in sources:
        try:
            d = fetch_icon_d2(lat, lon, date)
            h = d.get("hourly", {})
            hourly_icon = h
            at13 = {}
            for p in ICON_D2_PARAMS:
                at13[p] = _get_at_hour(h, "13:00", p)
            result["sources"]["icon_d2"] = {"at_13": at13, "hourly": h}
        except Exception as e:
            result["sources"]["icon_d2"] = {"error": str(e)}

    # --- GFS ---
    if "gfs" in sources:
        try:
            d = fetch_gfs(lat, lon, date)
            h = d.get("hourly", {})
            hourly_gfs = h
            at13 = {}
            for p in GFS_PARAMS:
                at13[p] = _get_at_hour(h, "13:00", p)
            result["sources"]["gfs"] = {"at_13": at13, "hourly": h}
        except Exception as e:
            result["sources"]["gfs"] = {"error": str(e)}

    # --- Multi-model ---
    if "multimodel" in sources:
        try:
            d = fetch_multimodel(lat, lon, date)
            h = d.get("hourly", {})
            at13 = {}
            for key in h:
                if key != "time":
                    at13[key] = _get_at_hour(h, "13:00", key)
            result["sources"]["multimodel"] = {"at_13": at13}
        except Exception as e:
            result["sources"]["multimodel"] = {"error": str(e)}

    # --- GeoSphere NWP ---
    if "geosphere_nwp" in sources:
        try:
            d = fetch_geosphere_nwp(lat, lon)
            timestamps = d.get("timestamps", [])
            features = d.get("features", [])
            at12 = {}
            if features:
                params = features[0].get("properties", {}).get("parameters", {})
                for i, t in enumerate(timestamps):
                    if "12:00" in t or "13:00" in t:
                        for pname, pdata in params.items():
                            vals = pdata.get("data", [])
                            at12[pname] = vals[i] if i < len(vals) else None
                        u = at12.get("u10m")
                        v = at12.get("v10m")
                        if u is not None and v is not None:
                            ws, wd = wind_from_uv(u, v)
                            at12["wind_speed_10m"] = ws
                            at12["wind_dir_10m"] = wd
                        ug = at12.get("ugust")
                        vg = at12.get("vgust")
                        if ug is not None and vg is not None:
                            gs, _ = wind_from_uv(ug, vg)
                            at12["gust_speed"] = gs
                        break
            result["sources"]["geosphere_nwp"] = {"at_13_approx": at12, "time": "nearest 12/13 UTC"}
        except Exception as e:
            result["sources"]["geosphere_nwp"] = {"error": str(e)}

    # --- BrightSky ---
    if "brightsky" in sources:
        try:
            d = fetch_brightsky(lat, lon, date)
            weather = d.get("weather", [])
            at13 = {}
            for w in weather:
                if "13:00" in w.get("timestamp", ""):
                    at13 = w
                    break
            result["sources"]["brightsky"] = {"at_13": at13}
        except Exception as e:
            result["sources"]["brightsky"] = {"error": str(e)}

    # --- DWD MOSMIX ---
    if "mosmix" in sources and loc.get("mosmix_id"):
        try:
            d = fetch_mosmix(loc["mosmix_id"], date)
            if "error" not in d:
                at13 = {}
                for param_name, hourly_vals in d.get("hourly", {}).items():
                    at13[param_name] = hourly_vals.get("12:00") or hourly_vals.get("15:00")
                result["sources"]["mosmix"] = {
                    "model": "mosmix_l",
                    "station_name": d.get("station_name", "?"),
                    "at_13": at13,
                    "hourly": d.get("hourly", {}),
                }
            else:
                result["sources"]["mosmix"] = {"error": d["error"]}
        except Exception as e:
            result["sources"]["mosmix"] = {"error": str(e)}

    # --- –ü–æ—á–∞—Å–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ ---
    hourly_analysis = analyze_hourly_window(hourly_icon, hourly_gfs, loc)
    result["hourly_analysis"] = hourly_analysis

    # --- –û—Å–Ω–æ–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (ICON-D2 + GFS) ---
    icon = result["sources"].get("icon_d2", {}).get("at_13", {})
    gfs_data = result["sources"].get("gfs", {}).get("at_13", {})

    def _fb(d1, d2, key):
        """Fallback: d1[key] if not None, else d2[key]. Handles 0 correctly."""
        v = d1.get(key)
        return v if v is not None else d2.get(key)

    t2m = _fb(icon, gfs_data, "temperature_2m")
    td2m = _fb(icon, gfs_data, "dewpoint_2m")
    ws850 = _fb(icon, gfs_data, "windspeed_850hPa")
    ws700 = _fb(icon, gfs_data, "windspeed_700hPa")
    gusts = _fb(icon, gfs_data, "windgusts_10m")
    cape = _fb(icon, gfs_data, "cape")
    t850 = _fb(icon, gfs_data, "temperature_850hPa")
    t700 = _fb(icon, gfs_data, "temperature_700hPa")
    cloud = _fb(icon, gfs_data, "cloudcover")
    precip = _fb(icon, gfs_data, "precipitation")
    bl_h = gfs_data.get("boundary_layer_height")
    sw_rad = gfs_data.get("shortwave_radiation")
    li = gfs_data.get("lifted_index")
    cin = gfs_data.get("convective_inhibition")

    cloudbase_msl = None
    if t2m is not None and td2m is not None:
        cloudbase_msl = round(estimate_cloudbase_msl(t2m, td2m, loc["elev"]))

    lr = None
    if t850 is not None and t700 is not None:
        lr = round(lapse_rate(t850, t700), 1)

    wstar = None
    if bl_h is not None and sw_rad is not None and t2m is not None:
        wstar = estimate_wstar(bl_h, sw_rad, t2m)

    base_margin = None
    if cloudbase_msl is not None:
        base_margin = cloudbase_msl - loc["peaks"]

    assessment = {
        "temp_2m": t2m,
        "dewpoint_2m": td2m,
        "cloudbase_msl": cloudbase_msl,
        "base_margin_over_peaks": base_margin,
        "wind_850hPa_ms": ws850,
        "wind_700hPa_ms": ws700,
        "gusts_10m_ms": gusts,
        "cape_J_per_kg": cape,
        "cin_J_per_kg": cin,
        "lifted_index": li,
        "boundary_layer_height_m": bl_h,
        "lapse_rate_C_per_km": lr,
        "wstar_ms": wstar,
        "cloudcover_pct": cloud,
        "precipitation_mm": precip,
    }

    # –¢–µ—Ä–º–∏—á–µ—Å–∫–æ–µ –æ–∫–Ω–æ
    tw = hourly_analysis.get("thermal_window", {})
    assessment["thermal_window_start"] = tw.get("start")
    assessment["thermal_window_end"] = tw.get("end")
    assessment["thermal_window_hours"] = tw.get("duration_h", 0)
    assessment["thermal_window_peak"] = tw.get("peak_hour")

    # ‚îÄ‚îÄ –°—Ç–æ–ø-—Ñ–∏–ª—å—Ç—Ä—ã ‚îÄ‚îÄ
    flags = []
    if ws850 is not None and ws850 > 5.0:
        flags.append(("WIND_850", f"{ws850:.1f} m/s > 5.0 (–ø–æ—Ä–æ–≥ –¥–ª—è closed route)"))
    if gusts is not None and gusts > 10.0:
        flags.append(("GUSTS", f"{gusts:.1f} m/s > 10.0"))
    if base_margin is not None and base_margin < 1000:
        flags.append(("LOW_BASE", f"{cloudbase_msl}m MSL, margin {base_margin}m < 1000m over {loc['peaks']}m peaks"))
    if precip is not None and precip > 0.5:
        flags.append(("PRECIP", f"{precip:.1f} mm/h @13:00"))
    if cloud is not None and cloud > 80:
        flags.append(("OVERCAST", f"{cloud:.0f}%"))
    if lr is not None and lr < 5.5:
        flags.append(("STABLE", f"lapse rate {lr}¬∞C/km < 5.5 (weak thermals)"))
    if cape is not None and cape > 1500:
        flags.append(("HIGH_CAPE", f"{cape} J/kg ‚Äî risk of overdevelopment/storms"))
    if li is not None and li < -4:
        flags.append(("VERY_UNSTABLE", f"LI={li} ‚Äî storm risk"))
    if tw.get("duration_h", 0) > 0 and tw["duration_h"] < 5:
        flags.append(("SHORT_WINDOW", f"thermal window {tw['duration_h']}h < 5h"))

    # ‚îÄ‚îÄ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö? ‚îÄ‚îÄ
    key_values = [t2m, ws850, cape, cloud, precip]
    n_available = sum(1 for v in key_values if v is not None)
    insufficient_data = n_available < 2  # need at least 2 of 5 key params

    if insufficient_data:
        flags.append(("INSUFFICIENT_DATA",
                      f"Only {n_available}/5 key params available (no reliable assessment)"))

    # ‚îÄ‚îÄ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚îÄ‚îÄ
    positives = []
    if not insufficient_data:
        if lr is not None and lr > 7.0:
            positives.append(("STRONG_LAPSE", f"{lr}¬∞C/km"))
        if cape is not None and 300 < cape < 1500:
            positives.append(("GOOD_CAPE", f"{cape} J/kg"))
        if bl_h is not None and bl_h > 1500:
            positives.append(("DEEP_BL", f"{bl_h}m"))
        if cloudbase_msl is not None and base_margin is not None and base_margin > 1500:
            positives.append(("HIGH_BASE", f"{cloudbase_msl}m MSL (+{base_margin}m over peaks)"))
        if tw.get("duration_h", 0) >= 7:
            positives.append(("LONG_WINDOW", f"{tw['duration_h']}h thermal window"))
        if cloud is not None and cloud < 30:
            positives.append(("CLEAR_SKY", f"{cloud:.0f}%"))
        if wstar is not None and wstar >= 1.0:
            positives.append(("GOOD_WSTAR", f"W*={wstar:.1f} m/s (thermal strength)"))

    # ‚îÄ‚îÄ Composite scoring ‚îÄ‚îÄ
    CRITICAL_TAGS = {"WIND_850", "GUSTS", "PRECIP"}
    QUALITY_TAGS = {"OVERCAST", "STABLE", "SHORT_WINDOW", "INSUFFICIENT_DATA"}

    n_critical = sum(1 for tag, _ in flags if tag in CRITICAL_TAGS)
    n_quality = sum(1 for tag, _ in flags if tag in QUALITY_TAGS)
    n_danger = sum(1 for tag, _ in flags if tag in ("HIGH_CAPE", "VERY_UNSTABLE"))
    n_base = sum(1 for tag, _ in flags if tag == "LOW_BASE")
    n_positive = len(positives)

    score = 0
    score -= n_critical * 3
    score -= n_base * 2
    score -= n_quality * 1
    score -= n_danger * 1
    score += n_positive * 2

    if score <= -5:
        status = "NO-GO"
    elif score <= -2:
        status = "UNLIKELY"
    elif score <= 1:
        status = "MAYBE"
    elif score <= 4:
        status = "GO"
    else:
        status = "STRONG"

    if n_critical >= 2 or (n_critical >= 1 and n_base >= 1):
        status = "NO-GO"
    elif n_critical >= 1 and status in ("GO", "STRONG"):
        status = "MAYBE"

    if insufficient_data:
        status = "NO DATA"

    assessment["flags"] = [{"tag": t, "msg": m} for t, m in flags]
    assessment["positives"] = [{"tag": t, "msg": m} for t, m in positives]
    assessment["score"] = score
    assessment["status"] = status
    result["assessment"] = assessment

    return result


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONSOLE OUTPUT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

STATUS_EMOJI = {
    "NO-GO": "üî¥", "UNLIKELY": "üü†", "MAYBE": "üü°",
    "GO": "üü¢", "STRONG": "üíö", "NO DATA": "‚ö™",
}

STATUS_ORDER = {"STRONG": 0, "GO": 1, "MAYBE": 2, "UNLIKELY": 3, "NO-GO": 4, "NO DATA": 5}


def print_triage(results: list, forecast_date: str = ""):
    """–ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ –≤ stdout."""
    print("\n" + "=" * 70)
    print(f"  QUICK TRIAGE ‚Äî Forecast for {forecast_date}")
    print("=" * 70)

    sorted_results = sorted(results, key=lambda r: STATUS_ORDER.get(
        r.get("assessment", {}).get("status", "NO-GO"), 5))

    for r in sorted_results:
        a = r.get("assessment", {})
        if "error" in r and "assessment" not in r:
            print(f"\n  {r['location']}: ERROR ‚Äî {r['error']}")
            continue

        status = a.get("status", "?")
        def _fv(v, unit="", prec=None):
            """Format value for console: None‚Üí'‚Äî', number‚Üíformatted."""
            if v is None:
                return "‚Äî"
            if isinstance(v, (int, float)) and prec is not None:
                return f"{v:.{prec}f}{unit}"
            return f"{v}{unit}"
        base = _fv(a.get("cloudbase_msl"), " m", 0)
        margin = _fv(a.get("base_margin_over_peaks"), " m", 0)
        ws850 = _fv(a.get("wind_850hPa_ms"), " m/s", 1)
        gusts_val = _fv(a.get("gusts_10m_ms"), " m/s", 1)
        cape_val = _fv(a.get("cape_J_per_kg"), " J/kg", 0)
        lr_val = _fv(a.get("lapse_rate_C_per_km"), " ¬∞C/km", 1)
        bl = _fv(a.get("boundary_layer_height_m"), " m", 0)
        li_val = _fv(a.get("lifted_index"))
        tw_start = a.get("thermal_window_start", "‚Äî")
        tw_end = a.get("thermal_window_end", "‚Äî")
        tw_h = a.get("thermal_window_hours", 0)

        emoji = STATUS_EMOJI.get(status, "‚ö™")

        print(f"\n  {emoji} {r['location']:15s} [{status}]  (score: {a.get('score', '?')})")
        print(f"     Base: {base} MSL (margin: {margin} over peaks)")
        print(f"     Wind @850hPa: {ws850}  |  Gusts: {gusts_val}")
        print(f"     CAPE: {cape_val}  |  LI: {li_val}  |  Lapse: {lr_val}  |  BL: {bl}")
        wstar_val = _fv(a.get("wstar_ms"), " m/s", 2)
        print(f"     W*: {wstar_val}  |  Cloud: {_fv(a.get('cloudcover_pct'), '%', 0)}")
        if tw_h > 0:
            print(f"     Thermal window: {tw_start}‚Äì{tw_end} ({tw_h}h)")
        else:
            print(f"     Thermal window: none detected")

        for f in a.get("flags", []):
            print(f"     ‚ö† {f['tag']}: {f['msg']}")
        for p in a.get("positives", []):
            print(f"     ‚úì {p['tag']}: {p['msg']}")

    # –ú—É–ª—å—Ç–∏–º–æ–¥–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    has_multi = any("multimodel" in r.get("sources", {}) for r in results)
    if has_multi:
        print(f"\n{'‚îÄ' * 70}")
        print("  MODEL COMPARISON @13:00")
        print(f"{'‚îÄ' * 70}")
        for r in sorted_results:
            mm = r.get("sources", {}).get("multimodel", {}).get("at_13", {})
            if mm:
                print(f"\n  {r['location']}:")
                models = ["icon_seamless", "gfs_seamless", "ecmwf_ifs025"]
                for param_base in ["temperature_2m", "cape", "cloudcover", "windspeed_10m"]:
                    vals = []
                    for m in models:
                        key = f"{param_base}_{m}"
                        v = mm.get(key)
                        vals.append(f"{v}" if v is not None else "‚Äî")
                    print(f"     {param_base:20s}  ICON={vals[0]:>8s}  GFS={vals[1]:>8s}  ECMWF={vals[2]:>8s}")

    print(f"\n{'=' * 70}\n")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MARKDOWN REPORT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _v(val, unit="", precision=1):
    """Format value for display."""
    if val is None:
        return "‚Äî"
    if isinstance(val, float):
        return f"{val:.{precision}f}{unit}"
    return f"{val}{unit}"


def generate_markdown_report(results: list, date: str, gen_time: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown-–æ—Ç—á—ë—Ç–∞."""
    lines = []
    lines.append(f"# ‚úàÔ∏è PG Weather Triage")
    lines.append("")
    lines.append(f"**Forecast for: {date}**")
    lines.append("")
    lines.append(f"*Generated: {gen_time}*")
    lines.append("")

    sorted_r = sorted(results, key=lambda r: STATUS_ORDER.get(
        r.get("assessment", {}).get("status", "NO-GO"), 5))

    # ‚îÄ‚îÄ Summary table ‚îÄ‚îÄ
    lines.append("## üìä Summary")
    lines.append("")
    lines.append("| Location | Drive | Status | Base @13 | Margin | Wind 850 | Gusts | CAPE | Lapse | BL | W* | Window |")
    lines.append("|----------|-------|--------|----------|--------|----------|-------|------|-------|----|-----|--------|")

    for r in sorted_r:
        a = r.get("assessment", {})
        s = a.get("status", "?")
        emoji = STATUS_EMOJI.get(s, "‚ö™")
        tw_h = a.get("thermal_window_hours", 0)
        tw_str = f"{a.get('thermal_window_start','')}-{a.get('thermal_window_end','')}" if tw_h > 0 else "‚Äî"
        lines.append(
            f"| {r['location']} | {_v(r.get('drive_h'),'h')} "
            f"| {emoji} **{s}** "
            f"| {_v(a.get('cloudbase_msl'),'m')} "
            f"| {_v(a.get('base_margin_over_peaks'),'m')} "
            f"| {_v(a.get('wind_850hPa_ms'),'m/s')} "
            f"| {_v(a.get('gusts_10m_ms'),'m/s')} "
            f"| {_v(a.get('cape_J_per_kg'),'',0)} "
            f"| {_v(a.get('lapse_rate_C_per_km'),'¬∞C/km')} "
            f"| {_v(a.get('boundary_layer_height_m'),'m',0)} "
            f"| {_v(a.get('wstar_ms'),'',2)} "
            f"| {tw_str} |"
        )
    lines.append("")

    # ‚îÄ‚îÄ Location details ‚îÄ‚îÄ
    lines.append("---")
    lines.append("")

    for r in sorted_r:
        a = r.get("assessment", {})
        s = a.get("status", "?")
        emoji = STATUS_EMOJI.get(s, "‚ö™")

        lines.append(f"## {emoji} {r['location']} ‚Äî **{s}** (score: {a.get('score','?')})")
        lines.append("")

        lines.append("### Key Metrics @13:00")
        loc_data = LOCATIONS.get(r.get("key"), {})
        lines.append(f"- **Cloud Base**: {_v(a.get('cloudbase_msl'),'m')} MSL "
                     f"(margin: {_v(a.get('base_margin_over_peaks'),'m')} over {loc_data.get('peaks','?')}m peaks)")
        lines.append(f"- **Wind @850hPa**: {_v(a.get('wind_850hPa_ms'),'m/s')}  |  "
                     f"**@700hPa**: {_v(a.get('wind_700hPa_ms'),'m/s')}")
        lines.append(f"- **Gusts**: {_v(a.get('gusts_10m_ms'),'m/s')}")
        lines.append(f"- **CAPE**: {_v(a.get('cape_J_per_kg'),'J/kg',0)}  |  "
                     f"**LI**: {_v(a.get('lifted_index'))}  |  "
                     f"**CIN**: {_v(a.get('cin_J_per_kg'),'J/kg',0)}")
        lines.append(f"- **Lapse rate**: {_v(a.get('lapse_rate_C_per_km'),'¬∞C/km')}  |  "
                     f"**BL height**: {_v(a.get('boundary_layer_height_m'),'m',0)}  |  "
                     f"**W***: {_v(a.get('wstar_ms'),' m/s',2)}")
        lines.append(f"- **Cloud cover**: {_v(a.get('cloudcover_pct'),'%',0)}  |  "
                     f"**Precip**: {_v(a.get('precipitation_mm'),'mm')}")
        tw_h = a.get("thermal_window_hours", 0)
        if tw_h > 0:
            lines.append(f"- **Thermal window**: {a.get('thermal_window_start')}‚Äì"
                         f"{a.get('thermal_window_end')} ({tw_h}h), peak @{a.get('thermal_window_peak')}")
        else:
            lines.append("- **Thermal window**: none detected")
        lines.append("")

        if a.get("flags"):
            lines.append("**‚ö† Warnings:**")
            for f in a["flags"]:
                lines.append(f"- {f['tag']}: {f['msg']}")
            lines.append("")

        if a.get("positives"):
            lines.append("**‚úì Positive indicators:**")
            for p in a["positives"]:
                lines.append(f"- {p['tag']}: {p['msg']}")
            lines.append("")

        # Hourly profile
        hp = r.get("hourly_analysis", {}).get("hourly_profile", [])
        if hp:
            lines.append("### Hourly Profile")
            lines.append("| Hour | T | Base MSL | Cloud | Precip | W850 | Gusts | Lapse | BL | CAPE | W* |")
            lines.append("|------|---|----------|-------|--------|------|-------|-------|----|------|-----|")
            for h_item in hp:
                lines.append(
                    f"| {h_item['hour']} "
                    f"| {_v(h_item['temp_2m'],'¬∞C')} "
                    f"| {_v(h_item['cloudbase_msl'],'m',0)} "
                    f"| {_v(h_item['cloudcover'],'%',0)} "
                    f"| {_v(h_item['precipitation'],'mm')} "
                    f"| {_v(h_item['wind_850'],'',1)} "
                    f"| {_v(h_item['gusts'],'',1)} "
                    f"| {_v(h_item['lapse_rate'],'',1)} "
                    f"| {_v(h_item['bl_height'],'',0)} "
                    f"| {_v(h_item['cape'],'',0)} "
                    f"| {_v(h_item.get('wstar'),'',2)} |"
                )
            lines.append("")

        # MOSMIX
        mosmix = r.get("sources", {}).get("mosmix", {})
        if mosmix and "error" not in mosmix:
            lines.append(f"### DWD MOSMIX ({mosmix.get('station_name','?')})")
            mh = mosmix.get("hourly", {})
            if mh:
                all_hours = sorted(set().union(*(v.keys() for v in mh.values())))
                target_hours = [h for h in all_hours if h in ("06:00", "09:00", "12:00", "15:00", "18:00")]
                if target_hours:
                    param_list = [p for p in MOSMIX_PARAMS_OF_INTEREST if p in mh]
                    lines.append("| Param | " + " | ".join(target_hours) + " |")
                    lines.append("|-------" + "|------" * len(target_hours) + "|")
                    for p in param_list[:12]:
                        vals = [_v(mh[p].get(h)) for h in target_hours]
                        lines.append(f"| {p} | " + " | ".join(vals) + " |")
            lines.append("")

        # Model comparison
        mm = r.get("sources", {}).get("multimodel", {}).get("at_13", {})
        if mm:
            lines.append("### Model Comparison @13:00")
            lines.append("| Parameter | ICON | GFS | ECMWF |")
            lines.append("|-----------|------|-----|-------|")
            models = ["icon_seamless", "gfs_seamless", "ecmwf_ifs025"]
            for param_base in ["temperature_2m", "dewpoint_2m", "cape", "cloudcover",
                               "windspeed_10m", "windgusts_10m", "precipitation"]:
                vals = [_v(mm.get(f"{param_base}_{m}")) for m in models]
                lines.append(f"| {param_base} | " + " | ".join(vals) + " |")
            lines.append("")

        lines.append("---")
        lines.append("")

    return "\n".join(lines)



# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# HTML VIEWER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def _render_viewer_html(report_map: dict) -> str:
    """Render viewer HTML with embedded report map."""
    template_path = Path(__file__).parent / "viewer_template.html"
    with open(template_path, "r", encoding="utf-8") as f:
        template = f.read()

    data_json = json.dumps(report_map, ensure_ascii=False)
    return template.replace("__REPORTS_DATA__", data_json)


def generate_viewer_html(reports_dir: Path):
    """Generate history viewer (index.html) embedding all JSON reports."""
    json_files = sorted(reports_dir.glob("*.json"), reverse=True)

    all_reports = {}
    for jf in json_files:
        try:
            with open(jf, "r", encoding="utf-8") as f:
                all_reports[jf.stem] = json.load(f)
        except Exception:
            continue

    if not all_reports:
        print("  No JSON reports found, skipping viewer generation", file=sys.stderr)
        return

    html = _render_viewer_html(all_reports)

    viewer_path = reports_dir / "index.html"
    with open(viewer_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"HTML viewer:     {viewer_path}", file=sys.stderr)


def generate_single_report_html(reports_dir: Path, report_key: str, report_data: dict):
    """Generate self-contained HTML for one report + stable latest.html alias."""
    html = _render_viewer_html({report_key: report_data})

    dated_path = reports_dir / f"{report_key}.html"
    with open(dated_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"HTML report:     {dated_path}", file=sys.stderr)

    latest_path = reports_dir / "latest.html"
    with open(latest_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"HTML latest:     {latest_path}", file=sys.stderr)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Main
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

ALL_SOURCES = ["icon_d2", "gfs", "multimodel", "geosphere_nwp", "brightsky", "mosmix"]
HEADLESS_SOURCES = ["meteo_parapente", "xccontest", "alptherm"]


def run_headless_scraper(date: str, locs: dict, headless_sources: list) -> list:
    """–ó–∞–ø—É—Å–∫ Deno-—Å–∫—Ä–∞–ø–µ—Ä–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö headless browser."""
    deno = shutil.which("deno")
    if not deno:
        print("  deno not found ‚Äî skipping headless sources", file=sys.stderr)
        return []

    scraper_path = Path(__file__).parent / "scraper.ts"
    if not scraper_path.exists():
        print(f"  {scraper_path} not found ‚Äî skipping headless sources", file=sys.stderr)
        return []

    locations_json = json.dumps(
        [{"key": k, "lat": v["lat"], "lon": v["lon"]} for k, v in locs.items()]
    )

    cmd = [
        deno, "run", "-A", str(scraper_path),
        "--date", date,
        "--locations", locations_json,
        "--sources", ",".join(headless_sources),
    ]

    print(f"\nRunning headless scraper: {', '.join(headless_sources)}...", file=sys.stderr)
    try:
        proc = subprocess.run(
            cmd, capture_output=True, text=True, timeout=180
        )
        if proc.stderr:
            for line in proc.stderr.strip().split("\n"):
                print(f"  [scraper] {line}", file=sys.stderr)
        if proc.returncode != 0:
            print(f"  Scraper exited with code {proc.returncode}", file=sys.stderr)
            return []
        if not proc.stdout.strip():
            return []
        return json.loads(proc.stdout.strip())
    except subprocess.TimeoutExpired:
        print("  Scraper timed out (180s)", file=sys.stderr)
        return []
    except Exception as e:
        print(f"  Scraper error: {e}", file=sys.stderr)
        return []


def main():
    parser = argparse.ArgumentParser(description="PG Weather Triage ‚Äî data fetcher & report generator")
    parser.add_argument("--date", default=None,
                        help="Forecast date YYYY-MM-DD (default: next Saturday)")
    parser.add_argument("--locations", default="all", help="Comma-separated keys or 'all'")
    parser.add_argument("--sources", default="all",
                        help=f"Comma-separated sources or 'all'. Available: {','.join(ALL_SOURCES)}")
    parser.add_argument("--output-dir", default="reports", help="Output directory (default: reports)")
    parser.add_argument("--no-markdown", action="store_true", help="Skip Markdown report generation")
    parser.add_argument("--no-viewer", action="store_true", help="Skip HTML viewer (index.html) generation")
    parser.add_argument("--no-scraper", action="store_true", help="Skip headless browser sources")
    parser.add_argument("--headless-sources", default="meteo_parapente",
                        help=f"Comma-separated headless sources. Available: {','.join(HEADLESS_SOURCES)}")
    args = parser.parse_args()

    forecast_date = args.date or _next_saturday()
    print(f"Forecast date: {forecast_date}", file=sys.stderr)

    sources = ALL_SOURCES if args.sources == "all" else [s.strip() for s in args.sources.split(",")]

    if args.locations == "all":
        locs = LOCATIONS
    else:
        keys = [k.strip() for k in args.locations.split(",")]
        locs = {k: LOCATIONS[k] for k in keys if k in LOCATIONS}
        if not locs:
            print(f"No valid locations. Available: {', '.join(LOCATIONS.keys())}", file=sys.stderr)
            sys.exit(1)

    now = datetime.now(tz=timezone.utc)
    gen_time = now.strftime("%Y-%m-%d %H:%M UTC")
    timestamp_suffix = now.strftime("%Y%m%d_%H%M")

    # –ü—Ä–æ–≥–Ω–æ–∑—ã
    results = []
    for key, loc in locs.items():
        print(f"\nFetching {loc['name']}...", file=sys.stderr)
        try:
            result = assess_location(key, loc, forecast_date, sources)
            results.append(result)
        except Exception as e:
            print(f"  ERROR for {loc['name']}: {e}", file=sys.stderr)
            results.append({"location": loc["name"], "key": key, "error": str(e),
                            "assessment": {"status": "ERROR", "score": -99}})

    # ‚îÄ‚îÄ Headless scraper ‚îÄ‚îÄ
    scraper_data = []
    if not args.no_scraper:
        hs = [s.strip() for s in args.headless_sources.split(",")]
        hs = [s for s in hs if s in HEADLESS_SOURCES]
        if hs:
            scraper_data = run_headless_scraper(forecast_date, locs, hs)
            # Merge scraper results into location results
            results_by_key = {r.get("key"): r for r in results}
            for sd in scraper_data:
                src = sd.get("source", "unknown")
                loc_key = sd.get("location")
                if loc_key in results_by_key:
                    results_by_key[loc_key].setdefault("sources", {})[src] = sd
                    continue

                # Some headless sources (e.g. ALPTHERM overview) are not tied to
                # a specific location. Attach them to all locations so the data
                # is retained in reports and visible in the viewer.
                for r in results:
                    r.setdefault("sources", {})[src] = sd

    # Console output
    print_triage(results, forecast_date)

    # Output directory
    out_dir = Path(args.output_dir)
    out_dir.mkdir(exist_ok=True)

    # ‚îÄ‚îÄ JSON ‚îÄ‚îÄ
    json_results = []
    for r in results:
        r_copy = dict(r)
        clean_sources = {}
        for src_name, src_data in r_copy.get("sources", {}).items():
            if isinstance(src_data, dict):
                # Keep hourly for MOSMIX (viewer uses it), strip raw hourly from others
                skip_keys = set() if src_name == "mosmix" else {"hourly"}
                clean = {k: v for k, v in src_data.items() if k not in skip_keys}
                clean_sources[src_name] = clean
            else:
                clean_sources[src_name] = src_data
        r_copy["sources"] = clean_sources
        json_results.append(r_copy)

    json_output = {
        "generated_at": gen_time,
        "forecast_date": forecast_date,
        "sources_used": sources,
        "locations": json_results,
    }

    # Filename: forecast-date + generation timestamp
    file_stem = f"{forecast_date}_{timestamp_suffix}"
    json_path = out_dir / f"{file_stem}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(json_output, f, ensure_ascii=False, indent=2)
    print(f"JSON report:     {json_path}", file=sys.stderr)

    # ‚îÄ‚îÄ Markdown ‚îÄ‚îÄ
    if not args.no_markdown:
        md_content = generate_markdown_report(results, forecast_date, gen_time)
        md_path = out_dir / f"{file_stem}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        print(f"Markdown report: {md_path}", file=sys.stderr)

    # ‚îÄ‚îÄ HTML Viewer ‚îÄ‚îÄ
    if not args.no_viewer:
        generate_viewer_html(out_dir)
        generate_single_report_html(out_dir, file_stem, json_output)


if __name__ == "__main__":
    main()
